{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import text,sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('all')\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True news\")\n",
    "true = pd.read_csv(\"Downloads/true.csv\")\n",
    "true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fake news\")\n",
    "fake = pd.read_csv(\"Downloads/fake.csv\")\n",
    "fake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new column for classification of true and fake news and naming it as label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true['label'] = 1\n",
    "fake['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining true and fake news to make the complete dataset to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([true, fake], ignore_index = True, sort = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of true and fake news\n",
    "print(\"Number of true news articles:\", len(true))\n",
    "print(\"Number of fake news articles:\", len(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of different types of subjects for news and their counts\n",
    "print(df.subject.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining necessay attributes into one attribute and dropping the ones that are not considered for\n",
    "#the classification\n",
    "df['text'] = df['subject'] + \" \" + df['title'] + \" \" + df['text']\n",
    "del df['title']\n",
    "del df['subject']\n",
    "del df['date']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for a random dataset\n",
    "example = df.text[5]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of HTML content if any from the example initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(example, \"html.parser\")\n",
    "example = soup.get_text()\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of punctuations and special characters from the example initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = re.sub('\\[[^]]*\\]', ' ', example)\n",
    "example = re.sub('[^a-zA-Z]',' ',example)  # replaces non-alphabets with spaces\n",
    "example = example.lower() # Converting from uppercase to lowercase\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of stopwords from the example initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")   \n",
    "from nltk.corpus import stopwords\n",
    "example = nltk.word_tokenize(example)\n",
    "example = [ word for word in example if not word in set(stopwords.words(\"english\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizing the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.WordNetLemmatizer()\n",
    "example = [ lemma.lemmatize(word) for word in example] \n",
    "\n",
    "example = \" \".join(example)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal in all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of HTML Contents\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removal of Punctuation Marks\n",
    "def remove_punctuations(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# Removal of Special Characters\n",
    "def remove_characters(text):\n",
    "    return re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "\n",
    "#Removal of stopwords \n",
    "def remove_stopwords_and_lemmatization(text):\n",
    "    final_text = []\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    for word in text:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            lemma = nltk.WordNetLemmatizer()\n",
    "            word = lemma.lemmatize(word) \n",
    "            final_text.append(word)\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "#Congregated removal function\n",
    "def cleaning(text):\n",
    "    text = remove_html(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_characters(text)\n",
    "    text = remove_stopwords_and_lemmatization(text)\n",
    "    return text\n",
    "\n",
    "#Apply function on text column\n",
    "df['text'] = df['text'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts().plot.barh(title= 'Frequency of true and fake news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words in fake and true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "text_len = df[df['label'] == 1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(text_len, color = 'Green')\n",
    "ax1.set_title('True news')\n",
    "text_len = df[df['label'] == 0]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(text_len, color = 'Red')\n",
    "ax2.set_title('Fake news')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud of fake and true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "print(\"TRUE NEWS\")\n",
    "plt.figure(figsize = (15,15))\n",
    "wc = WordCloud(max_words = 100 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(df[df.label == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "print(\"FAKE NEWS\")\n",
    "plt.figure(figsize = (15,15))\n",
    "wc = WordCloud(max_words = 100 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(df[df.label == 0].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 300 #max number of words allowed per news\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(max_features, output_dim=embed_size, input_length=maxlen, trainable=False))\n",
    "#LSTM \n",
    "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model.add(Dense(units = 32 , activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.3, epochs=10, batch_size=batch_size, shuffle=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of LSTM on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100 , \"%\")\n",
    "print(\"Accuracy of LSTM on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], label = \"Train\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label = \"Test\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label = \"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"Test\")\n",
    "plt.title(\"Loss\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test)\n",
    "print(classification_report(y_test, pred, target_names = ['Fake','True']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "model = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "model = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(model)\n",
    "model = layers.Bidirectional(layers.LSTM(64))(model)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(model)\n",
    "models = keras.Model(inputs, outputs)\n",
    "models.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = models.predict(X_test)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_test, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_test * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_test, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_test, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_test * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_test, y_pred):\n",
    "    precision = precision_m(y_test, y_pred)\n",
    "    recall = recall_m(y_test, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", f1_m, precision_m, recall_m])\n",
    "history = models.fit(X_train, y_train, validation_split=0.3, batch_size=batch_size, epochs=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the BiLSTM on Training Data is - \" , models.evaluate(X_train,y_train)[1]*100 , \"%\")\n",
    "print(\"Accuracy of the BiLSTM on Testing Data is - \" , models.evaluate(X_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], label = \"Train\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label = \"Test\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label = \"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"Test\")\n",
    "plt.title(\"Loss\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, f1_score, precision, recall = models.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
