{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 44,266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/sneha/Downloads/Deep Learning/FakeNewsDataset/preprocessed_text_w_labels.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "text=df.Body.values\n",
    "labels=df.Label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT2 tokenizer...\n",
      "Done Tokenizing\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "# Load the GPT2 tokenizer.\n",
    "print('Loading GPT2 tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', do_lower_case=True)\n",
    "#gpt2 - 12-layer, 768-hidden, 12-heads, 117M parameters.\n",
    "\n",
    "\n",
    "# Define PAD Token = EOS Token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in text:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Add special tokens 'EOS' and 'BOS'\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    # Encode function can also be used for the first 5 tasks\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,  # Sentence to encode.\n",
    "        add_special_tokens=True,  \n",
    "        max_length=512,  # Pad & truncate all sentences.\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "print('Done Tokenizing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   86,  2542,   302, 11894,  1182,  4055, 41477, 17710,   514,  8681,\n",
       "          3015,  1227,  3236, 20628,  3277,  5057,  1414,  1687,  2005,   869,\n",
       "          9068,  4055, 37437,   323,  2956,    70,  4466, 24596,  2864,  1394,\n",
       "          7786, 30355,   835,  1871, 41477,   514,  1128,   411,  1317,   502,\n",
       "          4584,  2740,   269,    65,  1986,  3277,  9859,  1327,  1627, 28062,\n",
       "          4341,  1099,    76,   461, 22581,  6274,    75,   474, 42357,  2743,\n",
       "          1441,  9912, 29445,  3462,  1099,    76,   461,  2221,  1333,  1208,\n",
       "         28062,  4466,  1907,   588,  2792, 25731,  2296,  3692,  1825,    72,\n",
       "           772,   645,   303,  2022,   369, 32383,  1742,  1923,  3164, 41477,\n",
       "          5380,  1394,  1630,  8681,   906,   312,   836,  1940, 19997, 41477,\n",
       "           765,  1263,  4466,  1253,   292,  2036,  2743,  4341, 43268,   635,\n",
       "           765,  7601,  1253,   292, 30745,   891,   641, 14130,  2743,  4341,\n",
       "          1430,  1104,  2711,  3062,   361,  2267,  1167,  5685,  1356,   333,\n",
       "          1171,  1535,  2858,  1805, 19997,  6863,   435,   961,    72,   481,\n",
       "           910,   467,  1253,   292, 30745,   891,   641, 14130,  2743,  4341,\n",
       "           767,  1411,   502,  4584,  8900,  1402,  3057,   298,    72,  3821,\n",
       "          4925,   269, 14272,    84,   531,  1430, 43268,   910,   326,  1576,\n",
       "           761,  1577,  1089,  1414,  2179,   271,   838,  1367,  1411,  9068,\n",
       "          4055, 17666,   766,  9377,  1785,    84,  1057,   613, 20106,  1637,\n",
       "           531,   502,  4584,  1871, 41477,  3015,  2739, 12738,  2022,   636,\n",
       "            72,  5057, 15643,  1192,  1687, 18708,  1607, 21190, 28062,  4466,\n",
       "         11807,   751,  1315, 12989,   838,   614,  1160, 12989,  3277,  5057,\n",
       "          1393,  3285,  1317,  1561,  9068,  2424, 43268,   514,  1128,   411,\n",
       "           474,   577,   746, 37593,  1636,   531,   269,    65, 37593,  1636,\n",
       "           531, 41477,  1687,  2855,   561,  1038,   343,  4326,  1181,  8804,\n",
       "          1315, 12989,  3432, 13294,   333,  1152,   957,  1192,  1687,  2005,\n",
       "          3990,  5527,   530,  1551,  9068,  2424,  2855,   356,   303,  1683,\n",
       "          1775,  1208,  3752,    72,  3821,  1128,   411,   892,   467,  1414,\n",
       "           582,    72,   582,    72,   614,  1282, 37593,  1636,   531, 41477,\n",
       "         16361,  1687,  2353,   363,  4094,   514,  1687, 18708,  1542,   614,\n",
       "          5750,  1707,    72,  1693,  3349,  3821, 10834,   279,  2518,   374,\n",
       "          4121,   635,  1104,  1687,  2855,  2274,  1816,   502,  4584,   787,\n",
       "          1598,  5243,  2720,  9468,   283, 26189,    75,  4975,   636,    72,\n",
       "          1690,   869,   561,  1353, 41477,  3161,  8846,  2864, 41477,  1582,\n",
       "            75,  1192, 26189,    75,  1430,  1612,  2057, 17977,  3821,  3342,\n",
       "          6924,   283,  6924,  1698,  1535, 49741, 10550,  4528,  3595,   595,\n",
       "         23117,   880,  1430,  1827, 20518,  1122,  3342,   761,    72, 43268,\n",
       "         15038,   374,  4121,  1027,  4528, 12738,  2022,  6919,   910,   905,\n",
       "         41477,   561,  1333,  1414,  1687, 18708,  5380,  4341,  2005,  1919,\n",
       "          1430,  3061,  3821, 41477,   743,  1011,   736,  5852,  3308,   265,\n",
       "          3015, 43268,   761,  1331,    85,  4466,  2948,  1089, 18325, 43268,\n",
       "           779, 17124,   363,  3308,   265, 41477,  7135,  4528,  1630,  4404,\n",
       "         14130,  2743, 30745,   891,   641,  1430,  1919,  4341,  6331,    75,\n",
       "         25731,  4320,   263,   613, 20106,  3181,  4416,   954,   380,  1751,\n",
       "         19997,   384,   457, 24419,  1234,  9960,  2864,  1033,   343,  3128,\n",
       "         29135,  2223,  9963,  2914,    85,   288, 22260,  1430,  1805,  1862,\n",
       "          2296,  3692, 29489,   899,   312,   670,  8749,   906,   312,   531,\n",
       "          2274, 17044,  2085,   363,   765,  1814,  4218,   502,    87,  7490,\n",
       "          4865,  3355,  2296,  3692,  1099,  1488,  9933,   648,  2477,  1037,\n",
       "          4320,   263,  1128,   411,  1915,  8482, 44852,   417,  1297,   269,\n",
       "            65,  2661,  2792, 25731,  1825,    72,  2134,  3355,  1814,   761,\n",
       "           288, 22260]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21st centuri wire say 21wire predict new year look ahead new hostag crisi underwaytoday iranian militari forc report two small riverin us navi boat seiz iranian water current held iran farsi island persian gulf total 10 us navi personnel nine men one woman detain iranian author navi stray us navi patrol boat persian gulf imag sourc usniaccord pentagon initi narr follow sailor train mission around noon et boat experienc mechan difficulti drift iranianclaim water detain iranian coast guard offici ad stori sinc slightli revis white hous spokesman josh earnest follow narrativeth 2 boat travel en rout kuwait bahrain stop detain iraniansaccord usni search rescu team harri truman strike group scrambl aid crew stop short crew taken iranian contact iran receiv assur crew vessel return promptli pentagon spokesman peter cook told ap accord persian gulf tv farsi island one iranian island persian gulf bushehr provinc iran irgc navi base island island area 025 km complet restrict public due top secret government activ accord nbc news us state depart touch tehran offici iranian recogn us navi stray cours mistak sailor releas within hour war inc cnn wolf blitzer wast time rampingup talk militari tension israelifinanc neocon senat tom cottonneocon stuntalreadi us media includ cnn fox run talk point could come wors time presid obama right tonight state union speech tri prove american peopl iran countri trust implement histor nuclear deal latest naval controversi also come day implement phase iran nuclear deal say coincid might naivethat said could gop israelalign member pentagon intellig establish help engin today bizarr minicrisi order help weaken usiran relat extens obama controversi iranian nuclear dealthi look like case evidenc quick appear israel lobbysponsor prowar us senat tom cotton r place cnn direct aggress us militari talk point live air stori broke today cotton photo left immedi call event hostil blame iran us boat drift iranian water blame crisi presid obama claim embolden iranian aggress cotton goe tell giant lie media handler cnn wolf blitzer even blink much less challeng cotton imaginari statement iranian larg respons kill american soldier iraq afghanistan cotton went threaten iran say sailor vessel need immedi releas releas iran nuclear deal go forward'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_dict['input_ids'].flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35,412 training samples\n",
      "8,854 validation samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16 #paper recommends 16/32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0388ebb9fba4b178f83e82fabd90366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=665.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d67060b8bff4cafb5ee18725b3867bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=548118077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load GPT2ForSequenceClassification, the pretrained GPT2 model with a single\n",
    "# linear classification layer on top.\n",
    "\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Config\n",
    "\n",
    "model_config = GPT2Config.from_pretrained('gpt2', num_labels=2)\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Run this model on GPU.\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "epochs = 4 #paper recommends 2/3/4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# To store a number of quantities such as training and validation loss,\n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model in train mode. Drpout and Batchnorm work differently during training\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        \n",
    "        output = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "        \n",
    "        loss, logits = output[:2]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            output = model(b_input_ids,\n",
    "                                   token_type_ids=None,\n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss,logits = output[:2]\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# Display the table.\n",
    "print(df_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training completed on HPCC due to large GPU requirements of Transformer models\n",
    "12G GPU k80 - 8 hours of training with batchsize -> 8 and token_length=512\n",
    "32G GPU v100 - 1.5 hours of training with batchsize -> 16 and token_length=512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats for GPT-2\n",
    "\n",
    "Training complete!\n",
    "Total training took 1:44:23 (h:mm:ss)\n",
    "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
    "epoch                                                                         \n",
    "1           3.93e-02     1.42e-02            1.0       0:24:27         0:01:40\n",
    "2           7.59e-03     9.30e-03            1.0       0:24:25         0:01:40\n",
    "3           3.21e-03     1.16e-02            1.0       0:24:26         0:01:40\n",
    "4           8.42e-04     1.16e-02            1.0       0:24:26         0:01:40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model starts to overfit after 2nd epoch as validation loss increases after two epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Tokenizer Outputs for BERT and GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'new', 'gp', '##u', '!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"I have a new GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'have', 'a', 'new', 'gp', '##u', '!', '[SEP]']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.encode(\"I have a new GPU!\", add_special_tokens=True)\n",
    "#tokenizer.decode(indexed_tokens)\n",
    "tokenizer.convert_ids_to_tokens(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'Ġhave', 'Ġa', 'Ġnew', 'ĠGPU', '!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer.tokenize(\"I have a new GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'Ġhave', 'Ġa', 'Ġnew', 'ĠGPU', '!']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.encode(\"I have a new GPU!\", add_special_tokens=True)\n",
    "#tokenizer.decode(indexed_tokens)\n",
    "tokenizer.convert_ids_to_tokens(indexed_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextClassfication",
   "language": "python",
   "name": "fakenews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
